"""
Response Generator Module

Responsibility: Convert Intent → Response String (with optional context from SessionMemory)
Nothing more.

Does NOT:
- Access audio
- Access triggers
- Control flow
- Call OutputSink or SpeechToText
- Maintain memory (SessionMemory is read-only input)
- Store internal state
- Modify SessionMemory
- Retry on failure
- Stream output (single response only)

This is where the LLM lives. Isolated. Contained. Labeled.

v4 Update:
- Accepts optional SessionMemory for context reference
- Can include recent interactions in prompt
- Never modifies memory
- Memory is read-only scratchpad
"""

from abc import ABC, abstractmethod
import logging
from typing import Optional

# === Logging ===
logger = logging.getLogger(__name__)


class ResponseGenerator(ABC):
    """
    Base class for response generation engines.
    
    Single responsibility: Convert Intent → Response string
    
    v4 Update: Optionally accepts SessionMemory for context reference
    - memory is read-only (never modified)
    - can reference recent interactions in prompt
    - explicit and visible in code
    """

    @abstractmethod
    def generate(self, intent, memory: Optional['SessionMemory'] = None) -> str:
        """
        Generate a response for the given intent.

        Args:
            intent: Intent object with:
                - intent_type (IntentType enum)
                - confidence (float 0.0-1.0)
                - raw_text (str: original user input)
            memory: Optional SessionMemory for context reference (read-only)

        Returns:
            Response string (plain text, no markdown, no special formatting)

        Raises:
            ValueError: If intent is invalid or None
        """
        pass


class LLMResponseGenerator(ResponseGenerator):
    """
    LLM-based response generator using local Qwen model.
    
    Hardcoded model + parameters for predictability:
    - Model: argo:latest (Qwen via Ollama)
    - Temperature: 0.7 (deterministic but creative)
    - Max tokens: 100 (keep responses short)
    - No streaming (single response)
    - Read-only memory access (can reference, never modify)
    - No tool calling
    - No function calling
    
    v4 Update:
    - Accepts optional SessionMemory for context
    - May reference recent interactions in prompt
    - Never modifies memory
    - Memory is explicit and visible
    """

    def __init__(self):
        """Initialize LLM connection."""
        try:
            import requests
        except ImportError:
            raise ImportError("requests not installed. Run: pip install requests")

        self.requests = requests
        
        # Hardcoded LLM endpoint
        self.ollama_url = "http://localhost:11434"
        self.model = "argo:latest"
        
        # Hardcoded generation parameters
        self.temperature = 0.85  # Increased for more personality and creativity (0.7 was too dry)
        self.max_tokens = 2000  # Plenty of room for full answers (2-3 min speech worth)
        
        self.logger = logger
        self.logger.info("[LLMResponseGenerator v4] Initialized")
        self.logger.debug(f"  Endpoint: {self.ollama_url}")
        self.logger.debug(f"  Model: {self.model}")
        self.logger.debug(f"  Temperature: {self.temperature}")
        self.logger.debug(f"  Max tokens: {self.max_tokens}")

    def generate(self, intent, memory: Optional['SessionMemory'] = None) -> str:
        """
        Generate a response using Qwen LLM with optional context from memory.

        Args:
            intent: Intent object (from IntentParser)
            memory: Optional SessionMemory for context (read-only)

        Returns:
            Response string generated by LLM

        Raises:
            ValueError: If intent is invalid
            RuntimeError: If LLM connection fails
        """
        if intent is None:
            raise ValueError("intent is None")

        # Extract intent information
        intent_type = intent.intent_type.value  # "greeting", "question", etc.
        raw_text = intent.raw_text  # Original user input
        confidence = intent.confidence

        self.logger.info(
            f"[generate] Intent: {intent_type} "
            f"(confidence={confidence:.2f}, text='{raw_text[:50]}')"
        )
        
        # Log memory state if available (read-only inspection)
        if memory is not None:
            self.logger.debug(f"[generate] Memory available: {memory}")
            self.logger.debug(f"[generate] Recent interactions: {memory.get_recent_count()}")
        else:
            self.logger.debug(f"[generate] No memory available")

        # Build prompt for LLM
        prompt = self._build_prompt(intent_type, raw_text, confidence, memory)
        self.logger.debug(f"[generate] Prompt: {prompt[:100]}...")

        # Call LLM
        try:
            response_text = self._call_llm(prompt)
            self.logger.info(f"[generate] Generated: '{response_text}'")
            return response_text

        except Exception as e:
            self.logger.error(f"[generate] LLM call failed: {e}")
            raise

    def _build_prompt(
        self,
        intent_type: str,
        raw_text: str,
        confidence: float,
        memory: Optional['SessionMemory'] = None
    ) -> str:
        """
        Build a prompt for the LLM based on intent and optional context.

        Args:
            intent_type: "greeting", "question", "command", "unknown"
            raw_text: Original user input
            confidence: Classification confidence
            memory: Optional SessionMemory for context (read-only)

        Returns:
            Prompt string for LLM
        """
        # Start with basic context from memory if available
        context = ""
        if memory is not None and not memory.is_empty():
            # Build context from recent interactions (read-only access)
            context_summary = memory.get_context_summary()
            if context_summary:
                context = f"Context from recent conversation:\n{context_summary}\n\n"
        
        # System personality: You are ARGO, a friendly and knowledgeable AI assistant.
        # You're conversational but intelligent, engaging but not verbose.
        # You explain things clearly with practical examples when useful.
        # You have personality without being over-the-top.
        
        # Different prompts based on intent type
        if intent_type == "greeting":
            prompt = (
                f"{context}"
                f"The user greeted you with: '{raw_text}'\n"
                f"You are ARGO, a friendly AI assistant. Respond with a warm, engaging greeting (one sentence).\n"
                f"Response:"
            )
        elif intent_type == "question":
            prompt = (
                f"{context}"
                f"The user asked: '{raw_text}'\n"
                f"You are ARGO. Answer the question thoroughly but conversationally. Aim for 2-3 sentences with clarity and depth.\n"
                f"If you don't know the answer, admit it honestly and suggest what they could try instead.\n"
                f"Response:"
            )
        elif intent_type == "command":
            prompt = (
                f"{context}"
                f"The user gave a command: '{raw_text}'\n"
                f"You are ARGO. Execute the command directly with personality. If it's a count, list, recitation, or performance, do it enthusiastically.\n"
                f"Respond with the action itself, not just acknowledgment.\n"
                f"Response:"
            )
        else:  # unknown
            prompt = (
                f"{context}"
                f"The user said: '{raw_text}'\n"
                f"You didn't understand. Politely ask for clarification (one sentence max).\n"
                f"Response:"
            )

        return prompt

    def _call_llm(self, prompt: str) -> str:
        """
        Call Qwen via Ollama endpoint.

        Args:
            prompt: Prompt string for LLM

        Returns:
            Generated response string

        Raises:
            RuntimeError: If connection fails or LLM returns error
        """
        try:
            # Make request to Ollama
            url = f"{self.ollama_url}/api/generate"
            
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,  # Single response, not streaming
                "temperature": self.temperature,
                "num_predict": self.max_tokens,
            }

            self.logger.debug(f"[_call_llm] Calling {url}")
            response = self.requests.post(url, json=payload, timeout=30)
            
            if response.status_code != 200:
                raise RuntimeError(
                    f"LLM returned status {response.status_code}: {response.text}"
                )

            # Parse response
            result = response.json()
            response_text = result.get("response", "").strip()

            if not response_text:
                raise RuntimeError("LLM returned empty response")

            # Enhance response quality
            response_text = self._enhance_response(response_text)

            return response_text

        except self.requests.exceptions.ConnectionError as e:
            raise RuntimeError(
                f"Failed to connect to Ollama at {self.ollama_url}. "
                f"Make sure Ollama is running: {e}"
            )
        except Exception as e:
            raise RuntimeError(f"LLM call failed: {e}")
    def _enhance_response(self, response_text: str) -> str:
        """
        Post-process response to ensure quality and personality.
        
        - Removes LLM artifacts (extra tokens, repeated lines)
        - Ensures minimum substance (not one-word answers)
        - Keeps first sentence if extremely long
        - Maintains natural conversation flow
        
        Args:
            response_text: Raw LLM response
            
        Returns:
            Enhanced response string
        """
        # Clean up common LLM artifacts
        response_text = response_text.strip()
        
        # Remove trailing common LLM patterns
        response_text = response_text.rstrip("...")
        response_text = response_text.rstrip(",")
        
        # If response is very short (single word/short phrase), it's probably too minimal
        # This is a fallback (shouldn't happen with good prompts, but just in case)
        if len(response_text.split()) < 3 and not response_text.endswith(("?", "!")):
            self.logger.debug(f"[_enhance_response] Response too short, accepting as-is: '{response_text}'")
        
        return response_text